{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 1. Stroke Prediction\n",
        "# ============================\n",
        "\n",
        "# Import required libraries for data manipulation and visualization\n",
        "\n",
        "import pandas as pd  # For handling data in tabular format (DataFrames)\n",
        "import numpy as np  # For numerical operations and array handling\n",
        "\n",
        "import matplotlib.pyplot as plt  # For creating static visualizations\n",
        "import seaborn as sns  # For enhanced statistical data visualization\n",
        "\n",
        "# Import scikit-learn modules for model building and evaluation\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score  # For splitting data and cross-validation\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression  # Logistic Regression model\n",
        "from sklearn.tree import DecisionTreeClassifier  # Decision Tree model\n",
        "from sklearn.ensemble import RandomForestClassifier  # Random Forest model\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix  # For evaluating model performance\n",
        "\n",
        "# Import SMOTE from imbalanced-learn to handle class imbalance\n",
        "from imblearn.over_sampling import SMOTE  # Synthetic Minority Over-sampling Technique to balance minority class"
      ],
      "metadata": {
        "id": "tKAUOi7Mha-Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 2. Load and Inspect Dataset\n",
        "# ============================================\n",
        "\n",
        "# Define the URL of the raw CSV file hosted on GitHub\n",
        "url = 'https://raw.githubusercontent.com/monirulislammd/CIND820-Big-Data-Analytics-Project/main/healthcare-dataset-stroke-data.csv'\n",
        "\n",
        "# Read the CSV file directly from the GitHub URL into a pandas DataFrame\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display dataset shape and data types\n",
        "print(\"\\nDataset Shape:\", df.shape, \"\\n\")  # Print number of rows and columns\n",
        "print(df.info())  # Show column names, non-null counts, and data types\n",
        "\n",
        "# Display first few rows of the dataset\n",
        "print(\"\\n Example Dataset:\")\n",
        "df.head()  # Preview the top 5 rows to understand structure and sample values"
      ],
      "metadata": {
        "id": "BFotYg3MTKXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 3. Data Cleaning and Preprocessing\n",
        "# ============================================\n",
        "\n",
        "# Check for missing values in each column\n",
        "print(\"Missing values per column:\\n\", df.isnull().sum())  # Identify columns with missing data\n",
        "\n",
        "# Impute missing BMI values using the median (robust to outliers)\n",
        "df['bmi'].fillna(df['bmi'].median(), inplace=True)  # Replace NaNs in 'bmi' with median value\n",
        "\n",
        "# Drop 'id' column (not useful for prediction)\n",
        "df.drop(columns=['id'], inplace=True, errors='ignore')  # Remove identifier column if present\n",
        "\n",
        "# Remove rows where gender is 'Other' to avoid one-hot encoding mismatch\n",
        "df.drop(df[df['gender'] == 'Other'].index, inplace=True)  # Drop rows with ambiguous gender category\n",
        "\n",
        "# Replace 'Unknown' in smoking_status with 'never smoked' for consistency\n",
        "df['smoking_status'] = df['smoking_status'].replace('Unknown', 'never smoked')  # Simplify category\n",
        "\n",
        "# Final check for missing values after cleaning\n",
        "print(\"\\nMissing values after imputation:\\n\", df.isnull().sum())  # Confirm all missing values handled\n",
        "\n",
        "# Display summary statistics of cleaned dataset\n",
        "print(\"\\n\\nSummary Statistics:\\n\")\n",
        "df.describe() # Show count, mean, std, min, max, and quartiles for numeric columns"
      ],
      "metadata": {
        "id": "IZuoKPvnUa-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 4. Handle Outliers (IQR Clipping)\n",
        "# =========================================\n",
        "\n",
        "# Plot histograms for key numeric features to understand their distributions\n",
        "df[['age', 'avg_glucose_level', 'bmi']].hist(figsize=(10,5))  # Create histograms for selected numeric columns\n",
        "plt.suptitle(\"Histograms of Numeric Features with outliers\\n\")  # Add a super title for the figure\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Loop through selected numerical columns to cap outliers\n",
        "for col in ['age', 'avg_glucose_level', 'bmi']:\n",
        "    Q1 = df[col].quantile(0.25)  # First quartile (25th percentile)\n",
        "    Q3 = df[col].quantile(0.75)  # Third quartile (75th percentile)\n",
        "    IQR = Q3 - Q1  # Interquartile range (spread of middle 50%)\n",
        "\n",
        "    # Define lower and upper bounds for acceptable values\n",
        "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "\n",
        "    # Cap values outside the bounds using np.clip\n",
        "    df[col] = np.clip(df[col], lower, upper)  # Values below 'lower' set to 'lower', above 'upper' set to 'upper'"
      ],
      "metadata": {
        "id": "dMxxV8kOh_HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 5. Exploratory Data Analysis (EDA)\n",
        "# =========================================\n",
        "\n",
        "# Plot histograms for key numeric features to understand their distributions\n",
        "df[['age', 'avg_glucose_level', 'bmi']].hist(figsize=(10,5))  # Create histograms for selected numeric columns\n",
        "plt.suptitle(\"Histograms of Numeric Features after removing outliers\\n\")  # Add a super title for the figure\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Define categorical columns to visualize their frequency distributions\n",
        "cat_cols = ['ever_married', 'gender', 'work_type', 'Residence_type', 'smoking_status']\n",
        "\n",
        "# Loop through each categorical column and plot a countplot\n",
        "\n",
        "print(\"\\n\\nFrequency Distribution of categorial features:\")\n",
        "for c in cat_cols:\n",
        "    sns.countplot(x=c, data=df)  # Bar chart showing frequency of each category\n",
        "    plt.title(f\"\\n\\nDistribution of {c}\\n\")  # Title for each plot\n",
        "    plt.show()  # Display the plot"
      ],
      "metadata": {
        "id": "DH9sKHe8iEmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# 6. Feature Encoding\n",
        "# ==========================\n",
        "\n",
        "# Encode binary categorical features to numeric format (0/1)\n",
        "df['ever_married'] = df['ever_married'].map({'No': 0, 'Yes': 1})  # Convert 'ever_married' to binary\n",
        "df['hypertension'] = df['hypertension'].map({0: 0, 1: 1})  # Already numeric, but ensures consistency\n",
        "df['heart_disease'] = df['heart_disease'].map({0: 0, 1: 1})  # Already numeric, but explicitly mapped\n",
        "\n",
        "# Apply one-hot encoding to multi-category features\n",
        "# drop_first=True avoids dummy variable trap (perfect multicollinearity)\n",
        "df = pd.get_dummies(\n",
        "    df,\n",
        "    columns=['gender', 'work_type', 'Residence_type', 'smoking_status'],\n",
        "    drop_first=True  # Drops first category to keep k-1 encoded columns\n",
        ")\n",
        "\n",
        "# Display first few rows of the encoded dataset\n",
        "print(\"Encoded Dataset:\\n\")\n",
        "df.head()  # Preview transformed dataset with encoded features"
      ],
      "metadata": {
        "id": "KFFBrzcsiRfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 7. Handle Class Imbalance (SMOTE)\n",
        "# ============================================\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('stroke', axis=1)  # Drop the target column to get feature matrix\n",
        "y = df['stroke']  # Target variable indicating stroke occurrence (0 = No, 1 = Yes)\n",
        "\n",
        "# Check class distribution before applying SMOTE\n",
        "print(\"Before balancing:\\n\\n\", y.value_counts(normalize=True))  # Show class proportions (e.g., 95% no stroke, 5% stroke)\n",
        "\n",
        "# Apply SMOTE to balance the dataset by oversampling the minority class\n",
        "sm = SMOTE(random_state=42)  # Initialize SMOTE with a fixed random seed for reproducibility\n",
        "X_res, y_res = sm.fit_resample(X, y)  # Generate synthetic samples for minority class\n",
        "\n",
        "# Check class distribution after applying SMOTE\n",
        "print(\"\\n\\nAfter balancing:\\n\\n\", y_res.value_counts(normalize=True))  # Should now be ~50/50 between classes"
      ],
      "metadata": {
        "id": "comuPtLuimaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 8. Model Comparison with Cross-Validation\n",
        "# ============================================\n",
        "\n",
        "# Define a dictionary of models to compare\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),  # Logistic Regression with increased max iterations\n",
        "    'Decision Tree': DecisionTreeClassifier(),  # Basic Decision Tree classifier\n",
        "    'Random Forest': RandomForestClassifier()  # Ensemble method using multiple decision trees\n",
        "}\n",
        "\n",
        "# Define evaluation metrics to use during cross-validation\n",
        "metrics = {\n",
        "    'Accuracy': 'accuracy',  # Overall correctness\n",
        "    'Precision': 'precision',  # True Positives / (True Positives + False Positives)\n",
        "    'Recall': 'recall',  # True Positives / (True Positives + False Negatives)\n",
        "    'F1 Score': 'f1'  # Harmonic mean of precision and recall\n",
        "}\n",
        "\n",
        "# Initialize an empty list to store results for each model\n",
        "results = []\n",
        "\n",
        "# Loop through each model and evaluate using 10-fold cross-validation\n",
        "for name, model in models.items():\n",
        "    model_result = {'Model': name}  # Store model name\n",
        "    for metric_name, metric in metrics.items():\n",
        "        # Perform 10-fold cross-validation using the specified metric\n",
        "        scores = cross_val_score(model, X_res, y_res, cv=10, scoring=metric)\n",
        "        # Store mean and standard deviation of the scores\n",
        "        model_result[f'Mean {metric_name}'] = np.mean(scores)\n",
        "        model_result[f'Std {metric_name}'] = np.std(scores)\n",
        "    results.append(model_result)  # Append results for this model\n",
        "\n",
        "# Convert results list to a DataFrame for easy viewing\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Sort models by highest mean F1 Score (most balanced metric)\n",
        "results_df = results_df.sort_values(by='Mean F1 Score', ascending=False)\n",
        "\n",
        "# Display cross-validation results\n",
        "print(\"===== Cross-Validation Results =====\\n\")\n",
        "print(results_df.round(4))\n",
        "\n",
        "# Visualize F1 Score comparison across models using a bar plot\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(data=results_df, x='Model', y='Mean F1 Score', palette='viridis')  # Create barplot\n",
        "plt.title('\\n\\nModel Comparison (Mean F1 Score - 10-Fold CV)\\n')\n",
        "plt.ylabel('Mean F1 Score')\n",
        "plt.ylim(0, 1)  # Set y-axis limits\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4bTTqGgviyoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 9. Train-Test Split & Best Model Training\n",
        "# =========================================\n",
        "\n",
        "# Split the balanced dataset into training and test sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_res, y_res, test_size=0.2, random_state=42, stratify=y_res  # Stratify ensures class balance in both sets\n",
        ")\n",
        "\n",
        "# Identify the model with the highest mean F1 Score from cross-validation results\n",
        "best_model_name = results_df.iloc[0]['Model']  # Select top-performing model\n",
        "print(f\"\\nBest Model: {best_model_name}\")  # Display chosen model\n",
        "\n",
        "# Retrieve the best model from the models dictionary\n",
        "best_model = models[best_model_name]  # Load model object\n",
        "\n",
        "# Train the best model on the training data\n",
        "best_model.fit(X_train, y_train)  # Fit model to training set\n",
        "\n",
        "# Predict stroke outcomes on the test set\n",
        "y_pred = best_model.predict(X_test)  # Generate predictions\n",
        "\n",
        "# Evaluate model performance on the test set\n",
        "print(\"\\nTest Set Evaluation:\\n\")\n",
        "print(confusion_matrix(y_test, y_pred))  # Show confusion matrix (TP, FP, FN, TN)\n",
        "print(classification_report(y_test, y_pred))  # Show precision, recall, F1 score, and support\n",
        "\n",
        "# Display feature importance if the model supports it (e.g., tree-based models)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    # Extract and sort feature importances\n",
        "    feat_imp = pd.Series(best_model.feature_importances_, index=X_res.columns).sort_values(ascending=False)\n",
        "    print(\"\\nTop Features with absolute proportions:\\n\\n\", feat_imp.head(10))  # Show top 10 raw importances\n",
        "\n",
        "    # Normalize importances to show percentage contribution\n",
        "    feat_imp = feat_imp / feat_imp.sum()\n",
        "\n",
        "    # Plot top 10 features by normalized importance\n",
        "    feat_imp.head(10).plot(kind='barh')  # Horizontal bar chart\n",
        "    plt.xlabel(\"Normalized Importance\")  # X-axis label\n",
        "    plt.title(\"\\nTop 10 Important Features with their percentage proportion\\n\")  # Chart title\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "YE45MTMRjOys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# 10. Interactive Stroke Prediction Tool\n",
        "# =========================================\n",
        "\n",
        "def predict_stroke_users_interactive(model, feature_info, training_columns):\n",
        "    \"\"\"\n",
        "    Interactive Stroke Prediction Tool consistent with training preprocessing.\n",
        "\n",
        "    Allows user to input feature values interactively and predicts stroke risk\n",
        "    using the trained model.\n",
        "\n",
        "    Parameters:\n",
        "        model : trained sklearn model\n",
        "        feature_info : dict of {feature_name: allowed_values or 'numeric'}\n",
        "        training_columns : list of columns after one-hot encoding\n",
        "    \"\"\"\n",
        "\n",
        "    # Welcome message and instructions\n",
        "    print(\"=== Stroke Prediction Tool (One-Hot Encoded) ===\")\n",
        "    print(\"\\nType 'stop' at any time to exit.\\n\")\n",
        "    print(\"Enter your data in the provided box \\n\")\n",
        "\n",
        "    while True:\n",
        "        user_data = {}  # Dictionary to store user inputs\n",
        "\n",
        "        # Collect user inputs for each feature\n",
        "        for feature, allowed in feature_info.items():\n",
        "            while True:\n",
        "                value = input(f\"{feature}: \")  # Prompt user for input\n",
        "                if value.lower() == 'stop':\n",
        "                    print(\"\\nExiting prediction tool.\")\n",
        "                    return  # Exit loop and function\n",
        "\n",
        "                # Handle numeric input\n",
        "                if allowed == 'numeric':\n",
        "                    try:\n",
        "                        user_data[feature] = float(value)  # Convert to float\n",
        "                        break\n",
        "                    except ValueError:\n",
        "                        print(\"Invalid input. Enter a numeric value.\")\n",
        "\n",
        "                # Handle binary yes/no input\n",
        "                elif allowed == ['yes', 'no']:\n",
        "                    if value.lower() in ['yes', 'no']:\n",
        "                        user_data[feature] = 1 if value.lower() == 'yes' else 0  # Encode as 1/0\n",
        "                        break\n",
        "                    else:\n",
        "                        print(\"Enter yes or no.\")\n",
        "\n",
        "                # Handle multi-category input\n",
        "                else:\n",
        "                    allowed_lower = [v.lower() for v in allowed]  # Normalize allowed values\n",
        "                    if value.lower() in allowed_lower:\n",
        "                        idx = allowed_lower.index(value.lower())\n",
        "                        user_data[feature] = allowed[idx]  # Store original case value\n",
        "                        break\n",
        "                    else:\n",
        "                        print(f\"Invalid input. Allowed: {allowed}\")\n",
        "\n",
        "        # Convert user input dictionary to DataFrame\n",
        "        new_data = pd.DataFrame([user_data])\n",
        "\n",
        "        # Identify multi-category features for one-hot encoding\n",
        "        categorical_features = [\n",
        "            f for f, allowed in feature_info.items()\n",
        "            if allowed != 'numeric' and allowed != ['yes', 'no']\n",
        "        ]\n",
        "\n",
        "        # Apply one-hot encoding to multi-category features\n",
        "        if categorical_features:\n",
        "            new_data_encoded = pd.get_dummies(new_data, columns=categorical_features, drop_first=True)\n",
        "        else:\n",
        "            new_data_encoded = new_data.copy()\n",
        "\n",
        "        # Align encoded input with training columns (fill missing with 0)\n",
        "        new_data_encoded = new_data_encoded.reindex(columns=training_columns, fill_value=0)\n",
        "\n",
        "        # Predict stroke class and probability\n",
        "        pred_class = model.predict(new_data_encoded)[0]  # Predicted label\n",
        "        pred_proba = model.predict_proba(new_data_encoded)[0]  # Probability scores\n",
        "\n",
        "        # Display prediction results\n",
        "        print(\"\\n\\n===== Prediction Result =====\")\n",
        "        print(f\"\\nPredicted Class: {pred_class}\")  # 0 = No Stroke, 1 = Stroke\n",
        "        print(f\"\\nProbability [Class 0, Class 1]: {pred_proba}\\n\")  # Confidence scores\n",
        "        print(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "\n",
        "# Define feature types and allowed values for user input\n",
        "feature_info = {\n",
        "    'gender': ['Male', 'Female', 'Other'],\n",
        "    'age': 'numeric',\n",
        "    'hypertension': ['yes', 'no'],\n",
        "    'heart_disease': ['yes', 'no'],\n",
        "    'ever_married': ['yes', 'no'],\n",
        "    'work_type': ['Private', 'Self-employed', 'Govt_job', 'Children', 'Never_worked'],\n",
        "    'Residence_type': ['Urban', 'Rural'],\n",
        "    'avg_glucose_level': 'numeric',\n",
        "    'bmi': 'numeric',\n",
        "    'smoking_status': ['formerly smoked', 'never smoked', 'smokes']\n",
        "}\n",
        "\n",
        "# Launch the interactive prediction tool\n",
        "predict_stroke_users_interactive(best_model, feature_info, X_res.columns)"
      ],
      "metadata": {
        "id": "lTGlc9GewTmy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}